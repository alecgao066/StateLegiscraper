{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkAbbn7adzRU",
    "outputId": "049748da-021f-4d02-9c94-f7821e4b6ff6"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pH5MP-TZd2Ce"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6IW-zdKd-Dh"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from string import punctuation\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IhbdZglfBgA"
   },
   "outputs": [],
   "source": [
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def lemmatize_sent(text):\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "            for word, tag in nltk.pos_tag(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yspbDhvfCH-",
    "outputId": "762ebd0c-f0ea-4ee6-9225-3cc13f04f3a1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "checkpoints = '/content/drive/MyDrive/colab_files/'\n",
    "if not os.path.exists(checkpoints):\n",
    "    os.makedirs(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4aTJ2j-gD6F"
   },
   "outputs": [],
   "source": [
    "def nv_preprocess(nv_json_path, trim=None):    \n",
    "    \"\"\"\n",
    "    Loads JSON into environment as dictionary\n",
    "    Preprocesses the raw PDF export from previously generated json    \n",
    "    Optional: Trims transcript to exclude list of those present and signature page/list of exhibits\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nv_json_path : STRING\n",
    "        Local path of nv_json generated by nv_pdftotext.\n",
    "    trim: TRUE/Default(NONE)\n",
    "        Provides option to trim transcript to spoken section and transcriber notes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Cleaned dictionary that excludes PDF formatting and (optional) front and back end \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    file_path = open(nv_json_path,'rb')\n",
    "    data = json.load(file_path)\n",
    "    \n",
    "    if trim:\n",
    "        for key in data:\n",
    "            if isinstance(data[key], str):\n",
    "                ##Removes list of attendees on front end\n",
    "                start_location = re.search(r\"(CHAIR.*[A-z]\\:|Chair.*[A-z]\\:)\", data[key]).start() #Chair speaks first\n",
    "                data[key] = data[key][start_location:] #Starts transcript from when Chair first speaks\n",
    "                ##Removes signature page after submission (RESPECTFULLY SUBMITTED)\n",
    "                end_location = re.search(r\"(Respectfully\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED\\:)\", data[key]).start() #Signature page starts with\n",
    "                data[key] = data[key][:end_location] #End transcript just before respectfully submitted            \n",
    "                ##PDF formatting\n",
    "                data[key] = re.sub(r\"Page\\s[0-9]{1,}\", \"\", data[key]) #Removes page number\n",
    "                data[key] = re.sub(r\"\\n\", \"\", data[key])\n",
    "                data[key] = data[key].strip()\n",
    "                data[key]=\" \".join(data[key].split())\n",
    "            elif isinstance(data[key], list):\n",
    "                for i in range(len(data[key])):\n",
    "                    start_location = re.search(r\"(CHAIR.*[A-z]\\:|Chair.*[A-z]\\:)\", data[key][i]).start() #Chair speaks first\n",
    "                    data[key][i] = data[key][i][start_location:] #Starts transcript from when Chair first speaks\n",
    "                    end_location = re.search(r\"(Respectfully\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED)\",\n",
    "                                             data[key][i]).start()  # Signature page starts with\n",
    "                    ##Removes signature page after submission (RESPECTFULLY SUBMITTED)\n",
    "                    # try:\n",
    "                    #     end_location = re.search(r\"(Respectfully\\sSUBMITTED\\:|RESPECTFULLY\\sSUBMITTED\\:)\", data[key][i]).start() #Signature page starts with\n",
    "                    # except:\n",
    "                    #     end_location = -1\n",
    "                    data[key][i] = data[key][i][:end_location] #End transcript just before respectfully submitted\n",
    "                    ##PDF formatting\n",
    "                    data[key][i] = re.sub(r\"Page\\s[0-9]{1,}\", \"\", data[key][i]) #Removes page number\n",
    "                    data[key][i] = re.sub(r\"\\n\", \"\", data[key][i])\n",
    "                    data[key][i] = data[key][i].strip()\n",
    "                    data[key][i]=\" \".join(data[key][i].split())\n",
    "            else:\n",
    "                print(\"Incompatible File\")\n",
    "\n",
    "        return(data)\n",
    "            \n",
    "    else:\n",
    "        for key in data:\n",
    "            if isinstance(data[key], str):          \n",
    "                ##PDF formatting\n",
    "                data[key] = re.sub(r\"Page\\s[0-9]{1,}\", \"\", data[key]) #Removes page number\n",
    "                data[key] = re.sub(r\"\\n\", \"\", data[key])\n",
    "                data[key] = data[key].strip()\n",
    "                data[key]=\" \".join(data[key].split())\n",
    "            elif isinstance(data[key], list):\n",
    "                for i in range(len(data[key])):      \n",
    "                    ##PDF formatting\n",
    "                    data[key][i] = re.sub(r\"Page\\s[0-9]{1,}\", \"\", data[key][i]) #Removes page number\n",
    "                    data[key][i] = re.sub(r\"\\n\", \"\", data[key][i])\n",
    "                    data[key][i] = data[key][i].strip()\n",
    "                    data[key][i]=\" \".join(data[key][i].split())\n",
    "            else:\n",
    "                print(\"Incompatible File\")\n",
    "\n",
    "        return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk2NnrGcgl_F"
   },
   "outputs": [],
   "source": [
    "file_name = \"nv_hhs_m_2021.json\"\n",
    "data = nv_preprocess(checkpoints + file_name, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now all documents within 1 month are saved together coresponding to 1 key.  json_split_by_date split the documents and save them into a dictionary where 1 document coresponding to 1 hearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSpuPbJwnuUm"
   },
   "outputs": [],
   "source": [
    "def json_split_by_date(json_file):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Local path of nv_json generated by nv_pdftotext.\n",
    "        Local path of cleaned nv_json file. \n",
    "    Returns\n",
    "    -------\n",
    "    A new json file with month as the keys. We can call new_json_file[month] if we want the transcripts of meetings for this month.\n",
    "    Eg: call new_json_file[4], we would get the transcripts for April.\n",
    "\n",
    "    \"\"\"\n",
    "    json_date = {}\n",
    "    month = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "    for key in json_file.keys():\n",
    "      for doc in json_file[key]:\n",
    "        rx = r'{0}[ ]([1-9]|[12][0-9]|3[01])[,][ ](2020|2021)'.format(month[int(key)-1])\n",
    "        match = re.search(rx, doc)\n",
    "        date = datetime.datetime.strptime(match.group(), '%B %d, %Y').date()\n",
    "        json_date[date] = doc\n",
    "    return json_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh4EzIc6uIHL"
   },
   "outputs": [],
   "source": [
    "data = json_split_by_date(data)\n",
    "# match = re.search(r'(January|February|March|April|May|June|July|August|September|October|November|December)[ ]([1-9]|[12][0-9]|3[01])[,][ ](2020|2021)', doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (refered from Shujie's text_analysis codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RKTZcQR3wlq"
   },
   "outputs": [],
   "source": [
    "raw = {}\n",
    "for i in data.keys():\n",
    "    raw[i] = json.dumps(data[i])\n",
    "\n",
    "# Break up the string into words and punctuation, and create a list of words and punctuation.\n",
    "text = {}\n",
    "for i in raw.keys():\n",
    "    text[i] = [word.lower() for word in nltk.word_tokenize(raw[i])]\n",
    "\n",
    "# Stopwords are non-content words that primarily has only grammatical function\n",
    "stopwords_en = set(nltk.corpus.stopwords.words('english'))\n",
    "text_no_stopwords = {}\n",
    "for i in text.keys():\n",
    "    text_no_stopwords[i] = [word for word in text[i] if word not in stopwords_en]\n",
    "\n",
    "# Remove the punctuations\n",
    "text_no_stopwords_punc = {}\n",
    "for i in text_no_stopwords.keys():\n",
    "    text_no_stopwords_punc[i] = [word for word in text_no_stopwords[i] if word not in punctuation]\n",
    "\n",
    "# Lemmatization\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "for i in text_no_stopwords.keys():\n",
    "    text_no_stopwords_punc[i] = lemmatize_sent(text_no_stopwords_punc[i])\n",
    "\n",
    "# Remove the line breaks\n",
    "text_no_stopwords_punc_lb={}\n",
    "for i in text_no_stopwords_punc.keys():\n",
    "    text_no_stopwords_punc_lb[i] = [word for word in text_no_stopwords_punc[i] if not word.startswith('\\\\n')] + \\\n",
    "                                   [word[2:] for word in text_no_stopwords_punc[i] if word.startswith('\\\\n')]\n",
    "# Why\n",
    "text_no_stopwords_punc_lb_lemma={}\n",
    "for i in text_no_stopwords_punc_lb.keys():\n",
    "    text_no_stopwords_punc_lb_lemma[i]=lemmatize_sent(text_no_stopwords_punc_lb[i])\n",
    "\n",
    "text_no_stopwords_punc_lb_lemma_md={}\n",
    "for i in text_no_stopwords_punc_lb_lemma.keys():\n",
    "    text_no_stopwords_punc_lb_lemma_md[i]=[word for word in text_no_stopwords_punc_lb_lemma[i] if nltk.pos_tag([word])[0][1] != 'MD' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQwOMaHVCLz4",
    "outputId": "53cc1797-c729-439a-ca42-e81d2a870d05"
   },
   "outputs": [],
   "source": [
    "for i in text_no_stopwords_punc_lb_lemma_md.keys():\n",
    "  print(text_no_stopwords_punc_lb_lemma_md[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency (TF) \n",
    "##### Word frequency within each document, same as word counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjx794GzKya8"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "textdist={}\n",
    "for i in text_no_stopwords_punc_lb_lemma_md.keys():\n",
    "    textdist[i] = FreqDist(text_no_stopwords_punc_lb_lemma_md[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YThHZT9eLiUl",
    "outputId": "64870cd8-cb5c-4b91-8321-253c697b078e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in textdist.keys():\n",
    "  for sent, f_table in textdist[i].items():\n",
    "    print(sent, f_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YISGl6E6Mbu9"
   },
   "outputs": [],
   "source": [
    "termdist = {}\n",
    "for i in textdist.keys():\n",
    "  count_words = len(textdist[i].keys())\n",
    "  termdist[i] = textdist[i]\n",
    "  for word, count in textdist[i].items():\n",
    "    termdist[i][word] = count / count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-TW_TkrP8LI"
   },
   "outputs": [],
   "source": [
    "for i in termdist.keys():\n",
    "  for sent, f_table in termdist[i].items():\n",
    "    print(sent, f_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document frequency\n",
    "##### IDF = log(Number of total documents/(1 + Occurence of the word in all documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZJ_aMD3Qi1u"
   },
   "outputs": [],
   "source": [
    "idfdist = {}\n",
    "\n",
    "for i in termdist.keys():\n",
    "  for word, count in termdist[i].items():\n",
    "    if word in idfdist:\n",
    "      idfdist[word] += 1\n",
    "    else:\n",
    "      idfdist[word] = 1\n",
    "doc_count = len(termdist.keys())\n",
    "for word, count in idfdist.items():\n",
    "  idfdist[word] = math.log(doc_count/(count+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XwwEActTU9w",
    "outputId": "cac408d2-2161-41df-800e-221946c51ec5"
   },
   "outputs": [],
   "source": [
    "for sent, f_table in idfdist.items():\n",
    "  print(sent, f_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Freqency - Inverse Document frequency\n",
    "##### TF-IDF = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXdQFFruULAg"
   },
   "outputs": [],
   "source": [
    "tfidfdist = {}\n",
    "for i in termdist.keys():\n",
    "  tfidfdist[i] = termdist[i]\n",
    "  for word, count in termdist[i].items():\n",
    "    tfidfdist[i][word] = count * idfdist[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tlajy89pUwP2",
    "outputId": "14cd12cc-56a0-445f-8cd0-0e807934531a"
   },
   "outputs": [],
   "source": [
    "for i in tfidfdist.keys():\n",
    "  for sent, f_table in tfidfdist[i].items():\n",
    "    print(sent, f_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort TF-IDF in each document\n",
    "##### TF-IDF = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ls04PNdVHWW"
   },
   "outputs": [],
   "source": [
    "sort_dict = {}\n",
    "for i in tfidfdist.keys():\n",
    "  sort_dict[i] = dict(sorted(tfidfdist[i].items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d7pmjsqWKAM",
    "outputId": "58fa2405-78b0-4620-aac5-899bea6bc4c6"
   },
   "outputs": [],
   "source": [
    "for i in sort_dict.keys():\n",
    "  for sent, f_table in sort_dict[i].items():\n",
    "    print(sent, f_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "re7H2rKDbSnf",
    "outputId": "c6b27d28-6320-4573-f7a5-73f728bee2df"
   },
   "outputs": [],
   "source": [
    "for i in sort_dict.keys():\n",
    "  print(i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
